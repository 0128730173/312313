{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compatibility with CLRS, we advise using Python 3.9 or later\n",
    "# install required packages (only need to run once)\n",
    "#!pip install -e .\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from ltfs.common.logic import less_than, if_else\n",
    "\n",
    "def compute_hessian(param, closure_loss):\n",
    "    hessian_entries = []\n",
    "    param_with_grad = [p for p in param if p.requires_grad]\n",
    "    grads = torch.autograd.grad(closure_loss, param_with_grad, create_graph=True)\n",
    "    for grad in grads:\n",
    "        for grad_i in grad.view(-1):\n",
    "            hessian_row_i = torch.autograd.grad(grad_i, param_with_grad, retain_graph=True)\n",
    "            hessian_row_i = [item.item() for sublist in hessian_row_i for item in sublist.view(-1)]\n",
    "            hessian_entries.append(hessian_row_i)\n",
    "    return torch.tensor(hessian_entries)\n",
    "\n",
    "class NewtonsMethod(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, damping=1e-2):\n",
    "        super(NewtonsMethod, self).__init__(params, {'lr': lr, 'damping': damping})\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None:\n",
    "            loss = closure()  # Just call the closure without any arguments\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            damping = group['damping']\n",
    "            param_with_grad = [p for p in group['params'] if p.requires_grad]\n",
    "\n",
    "            flat_grad = torch.hstack([p.grad.flatten() for p in param_with_grad])\n",
    "            hessian = compute_hessian(group['params'], loss)\n",
    "            hessian += torch.eye(hessian.size(0), device=hessian.device) * damping\n",
    "            hessian_inv = torch.linalg.solve(hessian, flat_grad)\n",
    "\n",
    "            i = 0\n",
    "            for p in param_with_grad:\n",
    "                delta_w = hessian_inv[i:i+p.numel()].view_as(p)\n",
    "                p.data -= lr * delta_w\n",
    "                i += p.numel()\n",
    "                \n",
    "        return loss\n",
    "\n",
    "def distance2ref(layer, layer_ref):\n",
    "    return sum([torch.norm(p - p_ref) for p, p_ref in zip(layer.parameters(), layer_ref.parameters())])\n",
    "\n",
    "d = 12 # input dimension\n",
    "# define some placeholder structure because the layer definition requires it (the data will be random though)\n",
    "index = {\"X\": slice(0, 1), \"Y\": slice(1, 2), \"Z\": slice(2, 3), \"W\": slice(3, 4), \"B_global\": slice(4, 5), \"B_local\": slice(6, 7), \"S\": slice(6, 12)}\n",
    "X = torch.randn(20000, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Using any first-order optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss 1.0344297701261573e-13, distance 4.771546425590996e-07\n",
      "iter 100, loss 4.433348204268193e-13, distance 1.4358682932081695e-05\n",
      "iter 200, loss 1.314595006619079e-12, distance 3.0306186728547305e-05\n",
      "iter 300, loss 2.533690824605958e-12, distance 4.679554079239725e-05\n",
      "iter 400, loss 4.067522577782731e-12, distance 6.363722684239774e-05\n",
      "iter 500, loss 5.900021529996412e-12, distance 8.073097445332383e-05\n",
      "iter 600, loss 8.022824302043332e-12, distance 9.801216503501996e-05\n",
      "iter 700, loss 1.0430615599977466e-11, distance 0.00011545012658027968\n",
      "iter 800, loss 1.3119908758432733e-11, distance 0.00013301940722362075\n",
      "iter 900, loss 1.609022122627473e-11, distance 0.00015068731425805197\n"
     ]
    }
   ],
   "source": [
    "distance = 1e-8\n",
    "omega = 5e1\n",
    "layer = if_else(d, index, [\"X\"], [\"Y\"], \"Z\", [\"W\"], ifINF=omega)[0]\n",
    "layer_ref = if_else(d, index, [\"X\"], [\"Y\"], \"Z\", [\"W\"], ifINF=omega)[0]\n",
    "for p in layer.parameters():\n",
    "    p.data += torch.randn_like(p.data) * distance\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(layer.parameters(), lr=1e-8, weight_decay=1e-3)\n",
    "\n",
    "for iter in range(0, 1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.mse_loss(layer(X), layer_ref(X))\n",
    "    if iter != 0:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        loss.detach()\n",
    "\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"iter {iter}, loss {loss.item()}, distance {distance2ref(layer, layer_ref)}\")\n",
    "        if distance2ref(layer, layer_ref) < 1e-8:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Using a second-order method (Newton's method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss 1.919412982515624e-13, distance 4.7118882865253995e-07\n",
      "iter 100, loss 2.5716272498403237e-14, distance 3.4379886766804754e-07\n",
      "iter 200, loss 3.445463288926409e-15, distance 3.166777272868899e-07\n",
      "iter 300, loss 4.616228258858022e-16, distance 3.1211491451722303e-07\n",
      "iter 400, loss 6.184823963151172e-17, distance 3.113663895159845e-07\n",
      "iter 500, loss 8.286444885022362e-18, distance 3.1120533682987424e-07\n",
      "iter 600, loss 1.1102318896171125e-18, distance 3.111434044693802e-07\n",
      "iter 700, loss 1.4875860031202636e-19, distance 3.111140009591099e-07\n",
      "iter 800, loss 1.9933300266410284e-20, distance 3.111031116586465e-07\n",
      "iter 900, loss 2.673735190333543e-21, distance 3.110958579090009e-07\n"
     ]
    }
   ],
   "source": [
    "distance = 1e-8\n",
    "omega = 5e1\n",
    "layer = if_else(d, index, [\"X\"], [\"Y\"], \"Z\", [\"W\"], ifINF=omega)[0]\n",
    "layer_ref = if_else(d, index, [\"X\"], [\"Y\"], \"Z\", [\"W\"], ifINF=omega)[0]\n",
    "y = layer_ref(X)\n",
    "\n",
    "for p in layer.parameters():\n",
    "    p.data += torch.randn_like(p.data) * distance\n",
    "    p.requires_grad = True\n",
    "\n",
    "def closure(params=None, i=None):\n",
    "    original_params = [p.clone() for p in layer.parameters()]\n",
    "    \n",
    "    if params is not None and i is not None:\n",
    "        list(layer.parameters())[i].data = params.data.clone()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = layer(X)\n",
    "    loss = F.mse_loss(output, y)\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    for p, orig_p in zip(layer.parameters(), original_params):\n",
    "        p.data = orig_p.data.clone()\n",
    "\n",
    "    return loss\n",
    "\n",
    "optimizer = NewtonsMethod(layer.parameters(), lr=1e-2, damping=1e-5)\n",
    "\n",
    "for iter in range(0, 1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = optimizer.step(closure)\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"iter {iter}, loss {loss.item()}, distance {distance2ref(layer, layer_ref)}\")\n",
    "        if distance2ref(layer, layer_ref) < 1e-8:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
